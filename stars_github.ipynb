{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "os.chdir(\"D:\\machine_learning\\stellar_classification\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "#device=torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clscvrt(v):\n",
    "    return np.where(cls==v)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_ID</th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run_ID</th>\n",
       "      <th>rerun_ID</th>\n",
       "      <th>cam_col</th>\n",
       "      <th>field_ID</th>\n",
       "      <th>spec_obj_ID</th>\n",
       "      <th>fiber_ID</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>MJD</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.237661e+18</td>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>3606</td>\n",
       "      <td>301</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>6.543777e+18</td>\n",
       "      <td>171</td>\n",
       "      <td>0.634794</td>\n",
       "      <td>5812</td>\n",
       "      <td>56354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.237665e+18</td>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>4518</td>\n",
       "      <td>301</td>\n",
       "      <td>5</td>\n",
       "      <td>119</td>\n",
       "      <td>1.176014e+19</td>\n",
       "      <td>427</td>\n",
       "      <td>0.779136</td>\n",
       "      <td>10445</td>\n",
       "      <td>58158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.237661e+18</td>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>3606</td>\n",
       "      <td>301</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>5.152200e+18</td>\n",
       "      <td>299</td>\n",
       "      <td>0.644195</td>\n",
       "      <td>4576</td>\n",
       "      <td>55592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.237663e+18</td>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>4192</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>214</td>\n",
       "      <td>1.030107e+19</td>\n",
       "      <td>775</td>\n",
       "      <td>0.932346</td>\n",
       "      <td>9149</td>\n",
       "      <td>58039</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.237680e+18</td>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>8102</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>137</td>\n",
       "      <td>6.891865e+18</td>\n",
       "      <td>842</td>\n",
       "      <td>0.116123</td>\n",
       "      <td>6121</td>\n",
       "      <td>56187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.237679e+18</td>\n",
       "      <td>39.620709</td>\n",
       "      <td>-2.594074</td>\n",
       "      <td>22.16759</td>\n",
       "      <td>22.97586</td>\n",
       "      <td>21.90404</td>\n",
       "      <td>21.30548</td>\n",
       "      <td>20.73569</td>\n",
       "      <td>7778</td>\n",
       "      <td>301</td>\n",
       "      <td>2</td>\n",
       "      <td>581</td>\n",
       "      <td>1.055431e+19</td>\n",
       "      <td>438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9374</td>\n",
       "      <td>57749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1.237679e+18</td>\n",
       "      <td>29.493819</td>\n",
       "      <td>19.798874</td>\n",
       "      <td>22.69118</td>\n",
       "      <td>22.38628</td>\n",
       "      <td>20.45003</td>\n",
       "      <td>19.75759</td>\n",
       "      <td>19.41526</td>\n",
       "      <td>7917</td>\n",
       "      <td>301</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>8.586351e+18</td>\n",
       "      <td>866</td>\n",
       "      <td>0.404895</td>\n",
       "      <td>7626</td>\n",
       "      <td>56934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1.237668e+18</td>\n",
       "      <td>224.587407</td>\n",
       "      <td>15.700707</td>\n",
       "      <td>21.16916</td>\n",
       "      <td>19.26997</td>\n",
       "      <td>18.20428</td>\n",
       "      <td>17.69034</td>\n",
       "      <td>17.35221</td>\n",
       "      <td>5314</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>308</td>\n",
       "      <td>3.112008e+18</td>\n",
       "      <td>74</td>\n",
       "      <td>0.143366</td>\n",
       "      <td>2764</td>\n",
       "      <td>54535</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1.237661e+18</td>\n",
       "      <td>212.268621</td>\n",
       "      <td>46.660365</td>\n",
       "      <td>25.35039</td>\n",
       "      <td>21.63757</td>\n",
       "      <td>19.91386</td>\n",
       "      <td>19.07254</td>\n",
       "      <td>18.62482</td>\n",
       "      <td>3650</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>131</td>\n",
       "      <td>7.601080e+18</td>\n",
       "      <td>470</td>\n",
       "      <td>0.455040</td>\n",
       "      <td>6751</td>\n",
       "      <td>56368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1.237661e+18</td>\n",
       "      <td>196.896053</td>\n",
       "      <td>49.464643</td>\n",
       "      <td>22.62171</td>\n",
       "      <td>21.79745</td>\n",
       "      <td>20.60115</td>\n",
       "      <td>20.00959</td>\n",
       "      <td>19.28075</td>\n",
       "      <td>3650</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>8.343152e+18</td>\n",
       "      <td>851</td>\n",
       "      <td>0.542944</td>\n",
       "      <td>7410</td>\n",
       "      <td>57104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             obj_ID       alpha      delta         u         g         r  \\\n",
       "0      1.237661e+18  135.689107  32.494632  23.87882  22.27530  20.39501   \n",
       "1      1.237665e+18  144.826101  31.274185  24.77759  22.83188  22.58444   \n",
       "2      1.237661e+18  142.188790  35.582444  25.26307  22.66389  20.60976   \n",
       "3      1.237663e+18  338.741038  -0.402828  22.13682  23.77656  21.61162   \n",
       "4      1.237680e+18  345.282593  21.183866  19.43718  17.58028  16.49747   \n",
       "...             ...         ...        ...       ...       ...       ...   \n",
       "99995  1.237679e+18   39.620709  -2.594074  22.16759  22.97586  21.90404   \n",
       "99996  1.237679e+18   29.493819  19.798874  22.69118  22.38628  20.45003   \n",
       "99997  1.237668e+18  224.587407  15.700707  21.16916  19.26997  18.20428   \n",
       "99998  1.237661e+18  212.268621  46.660365  25.35039  21.63757  19.91386   \n",
       "99999  1.237661e+18  196.896053  49.464643  22.62171  21.79745  20.60115   \n",
       "\n",
       "              i         z  run_ID  rerun_ID  cam_col  field_ID   spec_obj_ID  \\\n",
       "0      19.16573  18.79371    3606       301        2        79  6.543777e+18   \n",
       "1      21.16812  21.61427    4518       301        5       119  1.176014e+19   \n",
       "2      19.34857  18.94827    3606       301        2       120  5.152200e+18   \n",
       "3      20.50454  19.25010    4192       301        3       214  1.030107e+19   \n",
       "4      15.97711  15.54461    8102       301        3       137  6.891865e+18   \n",
       "...         ...       ...     ...       ...      ...       ...           ...   \n",
       "99995  21.30548  20.73569    7778       301        2       581  1.055431e+19   \n",
       "99996  19.75759  19.41526    7917       301        1       289  8.586351e+18   \n",
       "99997  17.69034  17.35221    5314       301        4       308  3.112008e+18   \n",
       "99998  19.07254  18.62482    3650       301        4       131  7.601080e+18   \n",
       "99999  20.00959  19.28075    3650       301        4        60  8.343152e+18   \n",
       "\n",
       "       fiber_ID  redshift  plate    MJD  class  \n",
       "0           171  0.634794   5812  56354      0  \n",
       "1           427  0.779136  10445  58158      0  \n",
       "2           299  0.644195   4576  55592      0  \n",
       "3           775  0.932346   9149  58039      0  \n",
       "4           842  0.116123   6121  56187      0  \n",
       "...         ...       ...    ...    ...    ...  \n",
       "99995       438  0.000000   9374  57749      0  \n",
       "99996       866  0.404895   7626  56934      0  \n",
       "99997        74  0.143366   2764  54535      0  \n",
       "99998       470  0.455040   6751  56368      0  \n",
       "99999       851  0.542944   7410  57104      0  \n",
       "\n",
       "[100000 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"star_classification.csv\")\n",
    "cls=np.array(list(set(df[\"class\"])))\n",
    "\n",
    "df[\"class\"]=df[\"class\"].apply(clscvrt, args=())\n",
    "cols=list(df.columns)\n",
    "cols[-5]=df.columns[-1]\n",
    "cols[-1]=\"class\"\n",
    "df=df[cols]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiran M R\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "train, valid = np.split(df.sample(frac=1), [int(0.8*len(df))])\n",
    "print(len(train),len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3aWmmwH-RNtL"
   },
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe, oversample=False):\n",
    "  X = dataframe[dataframe.columns[:-1]].values\n",
    "  y = dataframe[dataframe.columns[-1]].values\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  X = scaler.fit_transform(X)\n",
    "\n",
    "  if oversample:\n",
    "    ros = RandomOverSampler()\n",
    "    X, y = ros.fit_resample(X, y)\n",
    "\n",
    "  data = np.hstack((X, np.reshape(y, (-1, 1))))\n",
    "\n",
    "  return data, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j5GvQhCKRNtM"
   },
   "outputs": [],
   "source": [
    "train, X_train, y_train = scale_dataset(train, oversample=True)\n",
    "valid, X_valid, y_valid = scale_dataset(valid, oversample=False)\n",
    "data_tensor = torch.tensor(X_valid, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10 hidden layer model\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "#         # Define the layers\n",
    "#         self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden3 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden4 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden5 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden6 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden7 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden8 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden9 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden10 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.dropout = nn.Dropout(p=dropout_prob)\n",
    "#         self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "#         # Define activation function (e.g., ReLU)\n",
    "#         self.activation = nn.ReLU()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.activation(self.hidden1(x))\n",
    "#         x = self.activation(self.hidden2(x))\n",
    "#         x = self.activation(self.hidden3(x))\n",
    "#         x = self.activation(self.hidden4(x))\n",
    "#         x = self.activation(self.hidden5(x))\n",
    "#         x = self.activation(self.hidden6(x))\n",
    "#         x = self.activation(self.hidden7(x))\n",
    "#         x = self.activation(self.hidden8(x))\n",
    "#         x = self.activation(self.hidden9(x))\n",
    "#         x = self.activation(self.hidden10(x))\n",
    "#         #x = torch.softmax(self.output_layer(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.output(x)\n",
    "#         x=nn.Softmax(dim=1)(x)\n",
    "#         return x\n",
    "\n",
    "# # Define the input size, hidden size, and output size\n",
    "# input_size = len(df.columns)-1  # Change this to match the dimension of your input data)\n",
    "# hidden_size = 32  # Number of neurons in each hidden layer\n",
    "# output_size = 3  # Number of output nodes\n",
    "\n",
    "# # Create an instance of the neural network\n",
    "# model = NeuralNetwork(input_size, hidden_size, output_size,0.0)\n",
    "\n",
    "# # You can now use this model for training and making predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10 hidden layer model\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "#         # Define the layers\n",
    "#         self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden3 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden4 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden5 = nn.Linear(hidden_size, hidden_size)\n",
    "#         # self.hidden6 = nn.Linear(hidden_size, hidden_size)\n",
    "#         # self.hidden7 = nn.Linear(hidden_size, hidden_size)\n",
    "#         # self.hidden8 = nn.Linear(hidden_size, hidden_size)\n",
    "#         # self.hidden9 = nn.Linear(hidden_size, hidden_size)\n",
    "#         # self.hidden10 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.dropout = nn.Dropout(p=dropout_prob)\n",
    "#         self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "#         # Define activation function (e.g., ReLU)\n",
    "#         self.activation = nn.ReLU()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.activation(self.hidden1(x))\n",
    "#         x = self.activation(self.hidden2(x))\n",
    "#         x = self.activation(self.hidden3(x))\n",
    "#         x = self.activation(self.hidden4(x))\n",
    "#         x = self.activation(self.hidden5(x))\n",
    "#         # x = self.activation(self.hidden6(x))\n",
    "#         # x = self.activation(self.hidden7(x))\n",
    "#         # x = self.activation(self.hidden8(x))\n",
    "#         # x = self.activation(self.hidden9(x))\n",
    "#         # x = self.activation(self.hidden10(x))    \n",
    "#         x = self.dropout(x)\n",
    "#         x = self.output(x)\n",
    "#         x = F.softmax(x, dim=1)  # Use nn.functional.softmax\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# # Define the input size, hidden size, and output size\n",
    "# input_size = len(df.columns)-1  # Change this to match the dimension of your input data)\n",
    "# hidden_size = 64  # Number of neurons in each hidden layer\n",
    "# output_size = 3  # Number of output nodes\n",
    "\n",
    "# # Create an instance of the neural network\n",
    "# dropout_prob = 0.1\n",
    "# model = NeuralNetwork(input_size, hidden_size, output_size,dropout_prob).to(device)\n",
    "# loss_function = nn.CrossEntropyLoss()  # For classification, adjust for your specific problemx\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# one_hot_labels=np.eye(3)[y_train]\n",
    "# features_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "# labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "# c,m=0,0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels=np.eye(3)[y_train]\n",
    "features_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3965,  0.2924,  1.0957,  ...,  2.0102,  0.7766,  0.6371],\n",
       "        [-0.2614,  0.5550,  0.9703,  ..., -0.7092, -1.1697, -1.1901],\n",
       "        [-1.7000, -0.2057, -1.3028,  ..., -0.7866, -0.4566, -0.1734],\n",
       "        ...,\n",
       "        [-0.6379,  0.3237,  1.7551,  ..., -0.7892,  1.0371,  1.0173],\n",
       "        [-0.1091, -1.5297, -1.1863,  ..., -0.7886, -0.7676, -0.6127],\n",
       "        [ 1.6455, -1.7482,  0.0064,  ..., -0.7879, -0.6766, -0.4514]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train\n",
      " [1 0 2 ... 2 2 2] \n",
      "One_hot_labels\n",
      " [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_train\\n\",y_train,\"\\nOne_hot_labels\\n\",one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 hidden layer model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Define activation function (e.g., ReLU)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden1(x))\n",
    "        x = self.activation(self.hidden2(x))\n",
    "    \n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        x = F.softmax(x, dim=1)  # Use nn.functional.softmax.to(device)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = len(df.columns)-1  # Change this to match the dimension of your input data)\n",
    "hidden_size = 64  # Number of neurons in each hidden layer\n",
    "output_size = 3  # Number of output nodes\n",
    "dropout_prob = 0.19\n",
    "d_p = dropout_prob\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size,dropout_prob).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()  # For classification, adjdropout_probt for your specific problemx\n",
    "\n",
    "data_tensor=X_train\n",
    "learning_rate= 0.008\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "one_hot_labels=np.eye(3)[y_train]\n",
    "features_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "c,m=0,0\n",
    "tim = time.ctime()\n",
    "tim=tim[11:19]+ '_'+ tim[20:]\n",
    "name=\"model_data_\" + tim.replace(':', '.') + \".txt\"\n",
    "\n",
    "with open(name, \"a\") as f:\n",
    "    f.write(\"Dropout Probability = \" + str(dropout_prob) + \"\\n\")\n",
    "    f.write(\"learning rate = \" + str(learning_rate) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = time.ctime()\n",
    "tim=tim[11:19]+ '_'+ tim[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size =1024  # Adjust the batch size as needed\n",
    "shuffle = True   # Set to True if you want to shuffle the data during each epoch\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_epochs = 450  # Adjust the number of training epochs as needed\n",
    "# print(\"Multiplier:\", c+1)\n",
    "# for epoch in range(num_epochs-m):\n",
    "    \n",
    "#     model.train()  # Set the model to training mode\n",
    "\n",
    "#     for batch_data, batch_labels in dataloader:  # dataloader is a DataLoader object with your training data\n",
    "#         optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(batch_data)\n",
    "#         loss = loss_function(outputs, batch_labels)\n",
    "\n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Print or log the loss for this epoch\n",
    "#     print(f'Epoch [{c*num_epochs+m+1}/{num_epochs*(c+1)}], Loss: {loss.item()}')\n",
    "\n",
    "#     # After training, save the model or use it for predictions\n",
    "#     torch.save(model.state_dict(), 'your_model.pth')\n",
    "#     m+=1\n",
    "\n",
    "# #print(\"Actual no. of epoch =\",c*num_epochs+epoch+1)\n",
    "# c+=1\n",
    "# m=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tensor = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "# chumma = 0\n",
    "# Nt=10\n",
    "# for i in range(Nt):\n",
    "#     pred=model(data_tensor)\n",
    "#     pred=pred.to('cpu')\n",
    "#     pred=pred.detach().numpy()\n",
    "#     prediction=pred.copy()\n",
    "#     i=0\n",
    "#     ind=np.arange(3)\n",
    "#     index=[]\n",
    "#     for a in prediction:\n",
    "#         pred[i]=np.array([0,0,0])\n",
    "#         #print(a)\n",
    "#         max_ind=np.argmax(a)\n",
    "#         index.append(max_ind)\n",
    "\n",
    "#         #print(max_ind)\n",
    "#         pred[i][max_ind]=1\n",
    "#         #print(pred[i])\n",
    "#         i+=1\n",
    "        \n",
    "#     count=0\n",
    "#     for i in range(len(pred)):\n",
    "#         if index[i] == y_valid[i]:\n",
    "#             count+=1\n",
    "#     chumma += count/len(pred)\n",
    "#     print(count/len(pred))\n",
    "# print(f'Average is (%) = {chumma/Nt*100} (epoch = {c*num_epochs+m})')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplier: 1\n",
      "Epoch [1/450], Loss: 1.0726308822631836\n",
      "Epoch [2/450], Loss: 1.0536401271820068\n",
      "Epoch [3/450], Loss: 1.0166748762130737\n",
      "Epoch [4/450], Loss: 0.9990162253379822\n",
      "Epoch [5/450], Loss: 1.0147935152053833\n",
      "Epoch [6/450], Loss: 0.9844797253608704\n",
      "Epoch [7/450], Loss: 0.9663738012313843\n",
      "Epoch [8/450], Loss: 0.912004828453064\n",
      "Epoch [9/450], Loss: 0.9518927335739136\n",
      "Epoch [10/450], Loss: 0.9405859708786011\n",
      "Epoch [11/450], Loss: 0.8954501748085022\n",
      "Epoch [12/450], Loss: 0.9346120953559875\n",
      "Epoch [13/450], Loss: 0.89791339635849\n",
      "Epoch [14/450], Loss: 0.9211969971656799\n",
      "Epoch [15/450], Loss: 0.8859829902648926\n",
      "Epoch [16/450], Loss: 0.9072633385658264\n",
      "Epoch [17/450], Loss: 0.9209749102592468\n",
      "Epoch [18/450], Loss: 0.8855128288269043\n",
      "Epoch [19/450], Loss: 0.8519555330276489\n",
      "Epoch [20/450], Loss: 0.8750988841056824\n",
      "Average is (%) = 49.164 (epoch = 19)\n",
      "Epoch [21/450], Loss: 0.8344117403030396\n",
      "Epoch [22/450], Loss: 0.8501617312431335\n",
      "Epoch [23/450], Loss: 0.8681443929672241\n",
      "Epoch [24/450], Loss: 0.8161661624908447\n",
      "Epoch [25/450], Loss: 0.8359788060188293\n",
      "Epoch [26/450], Loss: 0.8241398334503174\n",
      "Epoch [27/450], Loss: 0.7983782887458801\n",
      "Epoch [28/450], Loss: 0.7973362803459167\n",
      "Epoch [29/450], Loss: 0.7786436080932617\n",
      "Epoch [30/450], Loss: 0.7953478693962097\n",
      "Epoch [31/450], Loss: 0.76262366771698\n",
      "Epoch [32/450], Loss: 0.7519970536231995\n",
      "Epoch [33/450], Loss: 0.7646909356117249\n",
      "Epoch [34/450], Loss: 0.7533267140388489\n",
      "Epoch [35/450], Loss: 0.7450432777404785\n",
      "Epoch [36/450], Loss: 0.7709172964096069\n",
      "Epoch [37/450], Loss: 0.7443931698799133\n",
      "Epoch [38/450], Loss: 0.7270534634590149\n",
      "Epoch [39/450], Loss: 0.7227497100830078\n",
      "Epoch [40/450], Loss: 0.7359822392463684\n",
      "Average is (%) = 84.47650000000002 (epoch = 39)\n",
      "Epoch [41/450], Loss: 0.7159515619277954\n",
      "Epoch [42/450], Loss: 0.6945755481719971\n",
      "Epoch [43/450], Loss: 0.7056635022163391\n",
      "Epoch [44/450], Loss: 0.703015148639679\n",
      "Epoch [45/450], Loss: 0.696113109588623\n",
      "Epoch [46/450], Loss: 0.6968815326690674\n",
      "Epoch [47/450], Loss: 0.722612738609314\n",
      "Epoch [48/450], Loss: 0.7019528150558472\n",
      "Epoch [49/450], Loss: 0.6804349422454834\n",
      "Epoch [50/450], Loss: 0.6954729557037354\n",
      "Epoch [51/450], Loss: 0.690865159034729\n",
      "Epoch [52/450], Loss: 0.6627717614173889\n",
      "Epoch [53/450], Loss: 0.6724085807800293\n",
      "Epoch [54/450], Loss: 0.6838297843933105\n",
      "Epoch [55/450], Loss: 0.658063530921936\n",
      "Epoch [56/450], Loss: 0.6830116510391235\n",
      "Epoch [57/450], Loss: 0.6819104552268982\n",
      "Epoch [58/450], Loss: 0.6641178727149963\n",
      "Epoch [59/450], Loss: 0.6868445873260498\n",
      "Epoch [60/450], Loss: 0.6678640842437744\n",
      "Average is (%) = 87.7415 (epoch = 59)\n",
      "Epoch [61/450], Loss: 0.6505734920501709\n",
      "Epoch [62/450], Loss: 0.6584540605545044\n",
      "Epoch [63/450], Loss: 0.6607827544212341\n",
      "Epoch [64/450], Loss: 0.6807721853256226\n",
      "Epoch [65/450], Loss: 0.6496166586875916\n",
      "Epoch [66/450], Loss: 0.64076828956604\n",
      "Epoch [67/450], Loss: 0.6375561356544495\n",
      "Epoch [68/450], Loss: 0.6805042624473572\n",
      "Epoch [69/450], Loss: 0.6659545302391052\n",
      "Epoch [70/450], Loss: 0.6415535807609558\n",
      "Epoch [71/450], Loss: 0.6559152603149414\n",
      "Epoch [72/450], Loss: 0.6609976291656494\n",
      "Epoch [73/450], Loss: 0.6491600275039673\n",
      "Epoch [74/450], Loss: 0.6558502316474915\n",
      "Epoch [75/450], Loss: 0.6283219456672668\n",
      "Epoch [76/450], Loss: 0.6490161418914795\n",
      "Epoch [77/450], Loss: 0.6596764326095581\n",
      "Epoch [78/450], Loss: 0.6635560393333435\n",
      "Epoch [79/450], Loss: 0.6418378949165344\n",
      "Epoch [80/450], Loss: 0.660217821598053\n",
      "Average is (%) = 89.605 (epoch = 79)\n",
      "Epoch [81/450], Loss: 0.6413763761520386\n",
      "Epoch [82/450], Loss: 0.6405652165412903\n",
      "Epoch [83/450], Loss: 0.6498829126358032\n",
      "Epoch [84/450], Loss: 0.6315993070602417\n",
      "Epoch [85/450], Loss: 0.6598060727119446\n",
      "Epoch [86/450], Loss: 0.6519189476966858\n",
      "Epoch [87/450], Loss: 0.6571707725524902\n",
      "Epoch [88/450], Loss: 0.6603298187255859\n",
      "Epoch [89/450], Loss: 0.6479297280311584\n",
      "Epoch [90/450], Loss: 0.6270346641540527\n",
      "Epoch [91/450], Loss: 0.6538674235343933\n",
      "Epoch [92/450], Loss: 0.6461936831474304\n",
      "Epoch [93/450], Loss: 0.6374484896659851\n",
      "Epoch [94/450], Loss: 0.6240296959877014\n",
      "Epoch [95/450], Loss: 0.6224145889282227\n",
      "Epoch [96/450], Loss: 0.6446073055267334\n",
      "Epoch [97/450], Loss: 0.6282666325569153\n",
      "Epoch [98/450], Loss: 0.6109862923622131\n",
      "Epoch [99/450], Loss: 0.6577315330505371\n",
      "Epoch [100/450], Loss: 0.6446083188056946\n",
      "Average is (%) = 90.79049999999998 (epoch = 99)\n",
      "Epoch [101/450], Loss: 0.650791347026825\n",
      "Epoch [102/450], Loss: 0.634814977645874\n",
      "Epoch [103/450], Loss: 0.6486893892288208\n",
      "Epoch [104/450], Loss: 0.6540691256523132\n",
      "Epoch [105/450], Loss: 0.6599606275558472\n",
      "Epoch [106/450], Loss: 0.6466633081436157\n",
      "Epoch [107/450], Loss: 0.6322186589241028\n",
      "Epoch [108/450], Loss: 0.640288770198822\n",
      "Epoch [109/450], Loss: 0.6347551941871643\n",
      "Epoch [110/450], Loss: 0.6222995519638062\n",
      "Epoch [111/450], Loss: 0.6254928112030029\n",
      "Epoch [112/450], Loss: 0.628255307674408\n",
      "Epoch [113/450], Loss: 0.6304086446762085\n",
      "Epoch [114/450], Loss: 0.6460223197937012\n",
      "Epoch [115/450], Loss: 0.6524530649185181\n",
      "Epoch [116/450], Loss: 0.6084775328636169\n",
      "Epoch [117/450], Loss: 0.6651806235313416\n",
      "Epoch [118/450], Loss: 0.6257075667381287\n",
      "Epoch [119/450], Loss: 0.6330766081809998\n",
      "Epoch [120/450], Loss: 0.6456355452537537\n",
      "Average is (%) = 91.73649999999999 (epoch = 119)\n",
      "Epoch [121/450], Loss: 0.6390050053596497\n",
      "Epoch [122/450], Loss: 0.6373870372772217\n",
      "Epoch [123/450], Loss: 0.6467647552490234\n",
      "Epoch [124/450], Loss: 0.6206710338592529\n",
      "Epoch [125/450], Loss: 0.652036190032959\n",
      "Epoch [126/450], Loss: 0.6205407977104187\n",
      "Epoch [127/450], Loss: 0.6434879302978516\n",
      "Epoch [128/450], Loss: 0.6415248513221741\n",
      "Epoch [129/450], Loss: 0.6541028022766113\n",
      "Epoch [130/450], Loss: 0.6314643621444702\n",
      "Epoch [131/450], Loss: 0.6508545875549316\n",
      "Epoch [132/450], Loss: 0.6271732449531555\n",
      "Epoch [133/450], Loss: 0.6452088952064514\n",
      "Epoch [134/450], Loss: 0.615742564201355\n",
      "Epoch [135/450], Loss: 0.6422121524810791\n",
      "Epoch [136/450], Loss: 0.6463322043418884\n",
      "Epoch [137/450], Loss: 0.6337680220603943\n",
      "Epoch [138/450], Loss: 0.6533734798431396\n",
      "Epoch [139/450], Loss: 0.6416498422622681\n",
      "Epoch [140/450], Loss: 0.6428978443145752\n",
      "Average is (%) = 92.49300000000001 (epoch = 139)\n",
      "Epoch [141/450], Loss: 0.6267102360725403\n",
      "Epoch [142/450], Loss: 0.6290003061294556\n",
      "Epoch [143/450], Loss: 0.6344578862190247\n",
      "Epoch [144/450], Loss: 0.6377323269844055\n",
      "Epoch [145/450], Loss: 0.6330122947692871\n",
      "Epoch [146/450], Loss: 0.6275462508201599\n",
      "Epoch [147/450], Loss: 0.6205345988273621\n",
      "Epoch [148/450], Loss: 0.6106125712394714\n",
      "Epoch [149/450], Loss: 0.6235287189483643\n",
      "Epoch [150/450], Loss: 0.6224781274795532\n",
      "Epoch [151/450], Loss: 0.6370955109596252\n",
      "Epoch [152/450], Loss: 0.6243732571601868\n",
      "Epoch [153/450], Loss: 0.6304774284362793\n",
      "Epoch [154/450], Loss: 0.6347876787185669\n",
      "Epoch [155/450], Loss: 0.633651852607727\n",
      "Epoch [156/450], Loss: 0.6365566849708557\n",
      "Epoch [157/450], Loss: 0.6040858030319214\n",
      "Epoch [158/450], Loss: 0.6407996416091919\n",
      "Epoch [159/450], Loss: 0.6357027292251587\n",
      "Epoch [160/450], Loss: 0.6574713587760925\n",
      "Average is (%) = 93.1745 (epoch = 159)\n",
      "Epoch [161/450], Loss: 0.6118282079696655\n",
      "Epoch [162/450], Loss: 0.6065323352813721\n",
      "Epoch [163/450], Loss: 0.6386631727218628\n",
      "Epoch [164/450], Loss: 0.5980278849601746\n",
      "Epoch [165/450], Loss: 0.6213233470916748\n",
      "Epoch [166/450], Loss: 0.641106128692627\n",
      "Epoch [167/450], Loss: 0.6410515308380127\n",
      "Epoch [168/450], Loss: 0.6358672976493835\n",
      "Epoch [169/450], Loss: 0.6209867000579834\n",
      "Epoch [170/450], Loss: 0.6144486665725708\n",
      "Epoch [171/450], Loss: 0.6301940083503723\n",
      "Epoch [172/450], Loss: 0.6234593987464905\n",
      "Epoch [173/450], Loss: 0.6138906478881836\n",
      "Epoch [174/450], Loss: 0.6384010314941406\n",
      "Epoch [175/450], Loss: 0.6264654994010925\n",
      "Epoch [176/450], Loss: 0.6263457536697388\n",
      "Epoch [177/450], Loss: 0.6335461735725403\n",
      "Epoch [178/450], Loss: 0.6419695019721985\n",
      "Epoch [179/450], Loss: 0.6270274519920349\n",
      "Epoch [180/450], Loss: 0.6150052547454834\n",
      "Average is (%) = 93.6335 (epoch = 179)\n",
      "Epoch [181/450], Loss: 0.6080683469772339\n",
      "Epoch [182/450], Loss: 0.6296992897987366\n",
      "Epoch [183/450], Loss: 0.6225795149803162\n",
      "Epoch [184/450], Loss: 0.6118298172950745\n",
      "Epoch [185/450], Loss: 0.6339237093925476\n",
      "Epoch [186/450], Loss: 0.6254369020462036\n",
      "Epoch [187/450], Loss: 0.6150977611541748\n",
      "Epoch [188/450], Loss: 0.6238545775413513\n",
      "Epoch [189/450], Loss: 0.6294365525245667\n",
      "Epoch [190/450], Loss: 0.625163733959198\n",
      "Epoch [191/450], Loss: 0.6289035677909851\n",
      "Epoch [192/450], Loss: 0.6231154799461365\n",
      "Epoch [193/450], Loss: 0.6438193917274475\n",
      "Epoch [194/450], Loss: 0.6458698511123657\n",
      "Epoch [195/450], Loss: 0.6174457669258118\n",
      "Epoch [196/450], Loss: 0.650688886642456\n",
      "Epoch [197/450], Loss: 0.6142144203186035\n",
      "Epoch [198/450], Loss: 0.6192458868026733\n",
      "Epoch [199/450], Loss: 0.6476694941520691\n",
      "Epoch [200/450], Loss: 0.6076053977012634\n",
      "Average is (%) = 93.96600000000002 (epoch = 199)\n",
      "Epoch [201/450], Loss: 0.6041133999824524\n",
      "Epoch [202/450], Loss: 0.6149628758430481\n",
      "Epoch [203/450], Loss: 0.6011719107627869\n",
      "Epoch [204/450], Loss: 0.6052221655845642\n",
      "Epoch [205/450], Loss: 0.61860191822052\n",
      "Epoch [206/450], Loss: 0.6121580600738525\n",
      "Epoch [207/450], Loss: 0.6210746765136719\n",
      "Epoch [208/450], Loss: 0.6307456493377686\n",
      "Epoch [209/450], Loss: 0.6321883201599121\n",
      "Epoch [210/450], Loss: 0.6215744018554688\n",
      "Epoch [211/450], Loss: 0.6281158328056335\n",
      "Epoch [212/450], Loss: 0.6127833127975464\n",
      "Epoch [213/450], Loss: 0.6140294075012207\n",
      "Epoch [214/450], Loss: 0.6050006747245789\n",
      "Epoch [215/450], Loss: 0.617512047290802\n",
      "Epoch [216/450], Loss: 0.6160648465156555\n",
      "Epoch [217/450], Loss: 0.5988664627075195\n",
      "Epoch [218/450], Loss: 0.6350169777870178\n",
      "Epoch [219/450], Loss: 0.6054562330245972\n",
      "Epoch [220/450], Loss: 0.6042720675468445\n",
      "Average is (%) = 94.0975 (epoch = 219)\n",
      "Epoch [221/450], Loss: 0.6382413506507874\n",
      "Epoch [222/450], Loss: 0.6290969848632812\n",
      "Epoch [223/450], Loss: 0.6278581619262695\n",
      "Epoch [224/450], Loss: 0.6213681101799011\n",
      "Epoch [225/450], Loss: 0.6001121997833252\n",
      "Epoch [226/450], Loss: 0.5998956561088562\n",
      "Epoch [227/450], Loss: 0.5990888476371765\n",
      "Epoch [228/450], Loss: 0.620877742767334\n",
      "Epoch [229/450], Loss: 0.6034294962882996\n",
      "Epoch [230/450], Loss: 0.6244081854820251\n",
      "Epoch [231/450], Loss: 0.6219968795776367\n",
      "Epoch [232/450], Loss: 0.6155580878257751\n",
      "Epoch [233/450], Loss: 0.5949620604515076\n",
      "Epoch [234/450], Loss: 0.6151642799377441\n",
      "Epoch [235/450], Loss: 0.6068782210350037\n",
      "Epoch [236/450], Loss: 0.6069304943084717\n",
      "Epoch [237/450], Loss: 0.6012799143791199\n",
      "Epoch [238/450], Loss: 0.6175751090049744\n",
      "Epoch [239/450], Loss: 0.6391440033912659\n",
      "Epoch [240/450], Loss: 0.5960671901702881\n",
      "Average is (%) = 94.03649999999999 (epoch = 239)\n",
      "Epoch [241/450], Loss: 0.6172329783439636\n",
      "Epoch [242/450], Loss: 0.6306021809577942\n",
      "Epoch [243/450], Loss: 0.6288923025131226\n",
      "Epoch [244/450], Loss: 0.6145645976066589\n",
      "Epoch [245/450], Loss: 0.608445405960083\n",
      "Epoch [246/450], Loss: 0.6051285862922668\n",
      "Epoch [247/450], Loss: 0.6356673240661621\n",
      "Epoch [248/450], Loss: 0.6082048416137695\n",
      "Epoch [249/450], Loss: 0.6174815893173218\n",
      "Epoch [250/450], Loss: 0.629918098449707\n",
      "Epoch [251/450], Loss: 0.6339687705039978\n",
      "Epoch [252/450], Loss: 0.6110682487487793\n",
      "Epoch [253/450], Loss: 0.6169224977493286\n",
      "Epoch [254/450], Loss: 0.6190704703330994\n",
      "Epoch [255/450], Loss: 0.6341444253921509\n",
      "Epoch [256/450], Loss: 0.6133492588996887\n",
      "Epoch [257/450], Loss: 0.6086973547935486\n",
      "Epoch [258/450], Loss: 0.6088955402374268\n",
      "Epoch [259/450], Loss: 0.6065625548362732\n",
      "Epoch [260/450], Loss: 0.6154621243476868\n",
      "Average is (%) = 93.737 (epoch = 259)\n",
      "Epoch [261/450], Loss: 0.6100510358810425\n",
      "Epoch [262/450], Loss: 0.6194014549255371\n",
      "Epoch [263/450], Loss: 0.5922308564186096\n",
      "Epoch [264/450], Loss: 0.6147797703742981\n",
      "Epoch [265/450], Loss: 0.5964499115943909\n",
      "Epoch [266/450], Loss: 0.5927630066871643\n",
      "Epoch [267/450], Loss: 0.6171119213104248\n",
      "Epoch [268/450], Loss: 0.6164264678955078\n",
      "Epoch [269/450], Loss: 0.6305803656578064\n",
      "Epoch [270/450], Loss: 0.6118338704109192\n",
      "Epoch [271/450], Loss: 0.5968325138092041\n",
      "Epoch [272/450], Loss: 0.6223647594451904\n",
      "Epoch [273/450], Loss: 0.6121996641159058\n",
      "Epoch [274/450], Loss: 0.6327162981033325\n",
      "Epoch [275/450], Loss: 0.6258704662322998\n",
      "Epoch [276/450], Loss: 0.6312602758407593\n",
      "Epoch [277/450], Loss: 0.6072965860366821\n",
      "Epoch [278/450], Loss: 0.6286876201629639\n",
      "Epoch [279/450], Loss: 0.5971961617469788\n",
      "Epoch [280/450], Loss: 0.5937241315841675\n",
      "Average is (%) = 93.277 (epoch = 279)\n",
      "Epoch [281/450], Loss: 0.602243185043335\n",
      "Epoch [282/450], Loss: 0.6206527352333069\n",
      "Epoch [283/450], Loss: 0.6225743293762207\n",
      "Epoch [284/450], Loss: 0.6145113706588745\n",
      "Epoch [285/450], Loss: 0.6140791773796082\n",
      "Epoch [286/450], Loss: 0.6164606213569641\n",
      "Epoch [287/450], Loss: 0.6065014004707336\n",
      "Epoch [288/450], Loss: 0.6166925430297852\n",
      "Epoch [289/450], Loss: 0.6144545078277588\n",
      "Epoch [290/450], Loss: 0.6089310646057129\n",
      "Epoch [291/450], Loss: 0.6082714796066284\n",
      "Epoch [292/450], Loss: 0.6064414381980896\n",
      "Epoch [293/450], Loss: 0.6001327633857727\n",
      "Epoch [294/450], Loss: 0.624579668045044\n",
      "Epoch [295/450], Loss: 0.6082901954650879\n",
      "Epoch [296/450], Loss: 0.5922372937202454\n",
      "Epoch [297/450], Loss: 0.6195306777954102\n",
      "Epoch [298/450], Loss: 0.5979306697845459\n",
      "Epoch [299/450], Loss: 0.6229522228240967\n",
      "Epoch [300/450], Loss: 0.6207125186920166\n",
      "Average is (%) = 92.831 (epoch = 299)\n",
      "Epoch [301/450], Loss: 0.6072244644165039\n",
      "Epoch [302/450], Loss: 0.6115921139717102\n",
      "Epoch [303/450], Loss: 0.6212848424911499\n",
      "Epoch [304/450], Loss: 0.6080029010772705\n",
      "Epoch [305/450], Loss: 0.6225338578224182\n",
      "Epoch [306/450], Loss: 0.6311936378479004\n",
      "Epoch [307/450], Loss: 0.5910691022872925\n",
      "Epoch [308/450], Loss: 0.6124425530433655\n",
      "Epoch [309/450], Loss: 0.6119817495346069\n",
      "Epoch [310/450], Loss: 0.616735577583313\n",
      "Epoch [311/450], Loss: 0.6053870916366577\n",
      "Epoch [312/450], Loss: 0.618213951587677\n",
      "Epoch [313/450], Loss: 0.6056944131851196\n",
      "Epoch [314/450], Loss: 0.6097763776779175\n",
      "Epoch [315/450], Loss: 0.6235572099685669\n",
      "Epoch [316/450], Loss: 0.5894479751586914\n",
      "Epoch [317/450], Loss: 0.603972315788269\n",
      "Epoch [318/450], Loss: 0.5916022062301636\n",
      "Epoch [319/450], Loss: 0.6348389983177185\n",
      "Epoch [320/450], Loss: 0.6173725724220276\n",
      "Average is (%) = 92.20400000000002 (epoch = 319)\n",
      "Epoch [321/450], Loss: 0.620055615901947\n",
      "Epoch [322/450], Loss: 0.6109076142311096\n",
      "Epoch [323/450], Loss: 0.6328885555267334\n",
      "Epoch [324/450], Loss: 0.588616669178009\n",
      "Epoch [325/450], Loss: 0.621001124382019\n",
      "Epoch [326/450], Loss: 0.5958732962608337\n",
      "Epoch [327/450], Loss: 0.6079860925674438\n",
      "Epoch [328/450], Loss: 0.6067162156105042\n",
      "Epoch [329/450], Loss: 0.598881721496582\n",
      "Epoch [330/450], Loss: 0.628962516784668\n",
      "Epoch [331/450], Loss: 0.6108588576316833\n",
      "Epoch [332/450], Loss: 0.608875036239624\n",
      "Epoch [333/450], Loss: 0.6279489398002625\n",
      "Epoch [334/450], Loss: 0.5930153727531433\n",
      "Epoch [335/450], Loss: 0.6202543377876282\n",
      "Epoch [336/450], Loss: 0.6019951105117798\n",
      "Epoch [337/450], Loss: 0.6048601865768433\n",
      "Epoch [338/450], Loss: 0.6072351932525635\n",
      "Epoch [339/450], Loss: 0.5945384502410889\n",
      "Epoch [340/450], Loss: 0.5987105369567871\n",
      "Average is (%) = 91.61749999999999 (epoch = 339)\n",
      "Epoch [341/450], Loss: 0.611607015132904\n",
      "Epoch [342/450], Loss: 0.5881180167198181\n",
      "Epoch [343/450], Loss: 0.6066601276397705\n",
      "Epoch [344/450], Loss: 0.5993728637695312\n",
      "Epoch [345/450], Loss: 0.6380951404571533\n",
      "Epoch [346/450], Loss: 0.6099079251289368\n",
      "Epoch [347/450], Loss: 0.5974271297454834\n",
      "Epoch [348/450], Loss: 0.6006245613098145\n",
      "Epoch [349/450], Loss: 0.6137544512748718\n",
      "Epoch [350/450], Loss: 0.6068859100341797\n",
      "Epoch [351/450], Loss: 0.6214757561683655\n",
      "Epoch [352/450], Loss: 0.6093264222145081\n",
      "Epoch [353/450], Loss: 0.6173710227012634\n",
      "Epoch [354/450], Loss: 0.6122996807098389\n",
      "Epoch [355/450], Loss: 0.6113020777702332\n",
      "Epoch [356/450], Loss: 0.6181286573410034\n",
      "Epoch [357/450], Loss: 0.6251460909843445\n",
      "Epoch [358/450], Loss: 0.6101721525192261\n",
      "Epoch [359/450], Loss: 0.6301760673522949\n",
      "Epoch [360/450], Loss: 0.5903465747833252\n",
      "Average is (%) = 90.90199999999999 (epoch = 359)\n",
      "Epoch [361/450], Loss: 0.6172589063644409\n",
      "Epoch [362/450], Loss: 0.5988038778305054\n",
      "Epoch [363/450], Loss: 0.6007123589515686\n",
      "Epoch [364/450], Loss: 0.6119564175605774\n",
      "Epoch [365/450], Loss: 0.6113065481185913\n",
      "Epoch [366/450], Loss: 0.6087024211883545\n",
      "Epoch [367/450], Loss: 0.5963402986526489\n",
      "Epoch [368/450], Loss: 0.6055834293365479\n",
      "Epoch [369/450], Loss: 0.6129326224327087\n",
      "Epoch [370/450], Loss: 0.622511625289917\n",
      "Epoch [371/450], Loss: 0.6169142127037048\n",
      "Epoch [372/450], Loss: 0.599320113658905\n",
      "Epoch [373/450], Loss: 0.6013293862342834\n",
      "Epoch [374/450], Loss: 0.5947829484939575\n",
      "Epoch [375/450], Loss: 0.6090224981307983\n",
      "Epoch [376/450], Loss: 0.5830563902854919\n",
      "Epoch [377/450], Loss: 0.5982254147529602\n",
      "Epoch [378/450], Loss: 0.5961698889732361\n",
      "Epoch [379/450], Loss: 0.5937873125076294\n",
      "Epoch [380/450], Loss: 0.5942658185958862\n",
      "Average is (%) = 90.4555 (epoch = 379)\n",
      "Epoch [381/450], Loss: 0.5997172594070435\n",
      "Epoch [382/450], Loss: 0.601837158203125\n",
      "Epoch [383/450], Loss: 0.614338755607605\n",
      "Epoch [384/450], Loss: 0.5987603068351746\n",
      "Epoch [385/450], Loss: 0.6014322638511658\n",
      "Epoch [386/450], Loss: 0.6143937706947327\n",
      "Epoch [387/450], Loss: 0.607744038105011\n",
      "Epoch [388/450], Loss: 0.611239492893219\n",
      "Epoch [389/450], Loss: 0.5876319408416748\n",
      "Epoch [390/450], Loss: 0.5992001891136169\n",
      "Epoch [391/450], Loss: 0.6133620142936707\n",
      "Epoch [392/450], Loss: 0.5939448475837708\n",
      "Epoch [393/450], Loss: 0.6006886959075928\n",
      "Epoch [394/450], Loss: 0.630102813243866\n",
      "Epoch [395/450], Loss: 0.5963513255119324\n",
      "Epoch [396/450], Loss: 0.6134781241416931\n",
      "Epoch [397/450], Loss: 0.5895596146583557\n",
      "Epoch [398/450], Loss: 0.6007469296455383\n",
      "Epoch [399/450], Loss: 0.5897369384765625\n",
      "Epoch [400/450], Loss: 0.6017041206359863\n",
      "Average is (%) = 89.9785 (epoch = 399)\n",
      "Epoch [401/450], Loss: 0.6065740585327148\n",
      "Epoch [402/450], Loss: 0.6075395345687866\n",
      "Epoch [403/450], Loss: 0.6011254787445068\n",
      "Epoch [404/450], Loss: 0.620060384273529\n",
      "Epoch [405/450], Loss: 0.6009171605110168\n",
      "Epoch [406/450], Loss: 0.5916814804077148\n",
      "Epoch [407/450], Loss: 0.5981059074401855\n",
      "Epoch [408/450], Loss: 0.6188730001449585\n",
      "Epoch [409/450], Loss: 0.6146990656852722\n",
      "Epoch [410/450], Loss: 0.6027202010154724\n",
      "Epoch [411/450], Loss: 0.6251986026763916\n",
      "Epoch [412/450], Loss: 0.6070475578308105\n",
      "Epoch [413/450], Loss: 0.6064792275428772\n",
      "Epoch [414/450], Loss: 0.5999761819839478\n",
      "Epoch [415/450], Loss: 0.5885615348815918\n",
      "Epoch [416/450], Loss: 0.5927714705467224\n",
      "Epoch [417/450], Loss: 0.6024048328399658\n",
      "Epoch [418/450], Loss: 0.6196310520172119\n",
      "Epoch [419/450], Loss: 0.5947335362434387\n",
      "Epoch [420/450], Loss: 0.6048023104667664\n",
      "Average is (%) = 89.60100000000001 (epoch = 419)\n",
      "Epoch [421/450], Loss: 0.6121858954429626\n",
      "Epoch [422/450], Loss: 0.6058863997459412\n",
      "Epoch [423/450], Loss: 0.6188622713088989\n",
      "Epoch [424/450], Loss: 0.604561448097229\n",
      "Epoch [425/450], Loss: 0.6169266700744629\n",
      "Epoch [426/450], Loss: 0.6188071966171265\n",
      "Epoch [427/450], Loss: 0.6082362532615662\n",
      "Epoch [428/450], Loss: 0.6254145503044128\n",
      "Epoch [429/450], Loss: 0.6025392413139343\n",
      "Epoch [430/450], Loss: 0.6116867661476135\n",
      "Epoch [431/450], Loss: 0.5958355069160461\n",
      "Epoch [432/450], Loss: 0.6038697361946106\n",
      "Epoch [433/450], Loss: 0.6313826441764832\n",
      "Epoch [434/450], Loss: 0.6276274919509888\n",
      "Epoch [435/450], Loss: 0.6225301027297974\n",
      "Epoch [436/450], Loss: 0.6391082406044006\n",
      "Epoch [437/450], Loss: 0.6014652848243713\n",
      "Epoch [438/450], Loss: 0.6253268122673035\n",
      "Epoch [439/450], Loss: 0.5998401045799255\n",
      "Epoch [440/450], Loss: 0.620458722114563\n",
      "Average is (%) = 89.18700000000001 (epoch = 439)\n",
      "Epoch [441/450], Loss: 0.6133558750152588\n",
      "Epoch [442/450], Loss: 0.6030453443527222\n",
      "Epoch [443/450], Loss: 0.6136525869369507\n",
      "Epoch [444/450], Loss: 0.607981264591217\n",
      "Epoch [445/450], Loss: 0.6225834488868713\n",
      "Epoch [446/450], Loss: 0.5847584009170532\n",
      "Epoch [447/450], Loss: 0.6022042632102966\n",
      "Epoch [448/450], Loss: 0.5968130826950073\n",
      "Epoch [449/450], Loss: 0.6157307028770447\n"
     ]
    }
   ],
   "source": [
    "data_tensor = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "num_epochs = 450  # Adjust the number of training epochs as needed\n",
    "print(\"Multiplier:\", c+1)\n",
    "\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "for batch_data, batch_labels in dataloader:  # dataloader is a DataLoader object with your training data\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(batch_data)\n",
    "    loss = loss_function(outputs, batch_labels)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    now_loss = loss.item()\n",
    "\n",
    "for epoch in range(1,num_epochs-m):\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_data, batch_labels in dataloader:  # dataloader is a DataLoader object with your training data\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_data)\n",
    "        loss = loss_function(outputs, batch_labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    pre_loss = now_loss\n",
    "    now_loss = loss.item()\n",
    "    \n",
    "    if now_loss > pre_loss:\n",
    "        learning_rate = learning_rate * 0.999\n",
    "        d_p = d_p * 1.005\n",
    "    else:\n",
    "        learning_rate = learning_rate * 1.0005\n",
    "        d_p = d_p * 0.9995\n",
    "\n",
    "\n",
    "    model.dropout.p = d_p\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    # model=new_model\n",
    "    # new_model = NeuralNetwork(input_size, hidden_size, output_size,d_p_new).to(device)\n",
    "    # # Copy the weights from the pre-trained model to the new model\n",
    "    # new_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # optimizer = torch.optim.SGD(new_model.parameters(), lr=lr_new)\n",
    "\n",
    "    # Print or log the loss for this epoch\n",
    "    curr_epo=c*num_epochs+m+1\n",
    "    print(f'Epoch [{curr_epo}/{num_epochs*(c+1)}], Loss: {loss.item()}')\n",
    "\n",
    "    # After training, save the model or use it for predictions\n",
    "    torch.save(model.state_dict(), 'your_model.pth')\n",
    "\n",
    "\n",
    "    if((curr_epo+1)%20<1):\n",
    "        ##data_tensor = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "\n",
    "        chumma = 0\n",
    "        Nt=10\n",
    "        for i in range(Nt):\n",
    "            pred=model(data_tensor)\n",
    "            pred=pred.to('cpu')\n",
    "            pred=pred.detach().numpy()\n",
    "            prediction=pred.copy()\n",
    "            i=0\n",
    "            ind=np.arange(3)\n",
    "            index=[]\n",
    "            for a in prediction:\n",
    "                pred[i]=np.array([0,0,0])\n",
    "                #print(a)\n",
    "                max_ind=np.argmax(a)\n",
    "                index.append(max_ind)\n",
    "\n",
    "                #print(max_ind)\n",
    "                pred[i][max_ind]=1\n",
    "                #print(pred[i])\n",
    "                i+=1\n",
    "                \n",
    "            count=0\n",
    "            for i in range(len(pred)):\n",
    "                if index[i] == y_valid[i]:\n",
    "                    count+=1\n",
    "            chumma += count/len(pred)\n",
    "            #print(count/len(pred))\n",
    "        print(f'Average is (%) = {chumma/Nt*100} (epoch = {c*num_epochs+m})')\n",
    "        with open(name, \"a\") as f:\n",
    "            f.write(f'Average accuracy is (%) = {chumma/Nt*100} (epoch = {c*num_epochs+m+1})' + \"\\n\")\n",
    "\n",
    "    m+=1\n",
    "\n",
    "#print(\"Actual no. of epoch =\",c*num_epochs+epoch+1)\n",
    "c+=1\n",
    "m=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.028\n",
    "dropout_prob = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
